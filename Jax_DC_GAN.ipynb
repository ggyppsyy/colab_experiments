{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Jax_DC_GAN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ggyppsyy/colab_experiments/blob/master/Jax_DC_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "9O-Uf1FB-2nn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#https://towardsdatascience.com/understanding-batch-normalization-with-examples-in-numpy-and-tensorflow-with-interactive-code-7f59bb126642\n",
        "#https://wiseodd.github.io/techblog/2016/07/16/convnet-conv-layer/\n",
        "#https://github.com/huyouare/CS231n/blob/master/assignment2/cs231n/im2col.py\n",
        "#https://colab.research.google.com/github/google/jax/blob/master/notebooks/neural_network_and_data_loading.ipynb?authuser=1&hl=en#scrollTo=7APc6tD7TiuZ\n",
        "#https://github.com/pytorch/examples/blob/master/dcgan/main.py\n",
        "#https://arxiv.org/abs/1511.06434"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9_F-zpxi7Be2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!pip install --upgrade -q https://storage.googleapis.com/jax-wheels/cuda$(echo $CUDA_VERSION | sed -e 's/\\.//' -e 's/\\..*//')/jaxlib-0.1.12-cp36-none-linux_x86_64.whl\n",
        "#!pip install --upgrade -q jax"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cUiA3LRTSrx2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uGKNFbHUj0ph",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def leaky_relu(x):\n",
        "    y1 = ((x > 0) * x)                                                 \n",
        "    y2 = ((x <= 0) * x * 0.01)                                         \n",
        "    return y1 + y2 \n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1. + np.exp(-x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7p6HdzaTAc6r",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def channel_normalization(b):\n",
        "    batch_mean = np.mean(b)\n",
        "    print(batch_mean)\n",
        "    batch_var = np.sum((b-batch_mean) ** 2, axis=0) / b.shape[0]\n",
        "    print(batch_var)\n",
        "    return (b-batch_mean) / ( (batch_var + 1e-8) ** 0.5 )\n",
        "\n",
        "def batch_normalization(batch):\n",
        "    print(batch.shape)\n",
        "    batch = batch.transpose(1, 0, 2, 3).astype(np.float)\n",
        "    print(batch.shape)\n",
        "    for i in range(batch.shape[0]):\n",
        "        batch[i] = channel_normalization(batch[i])\n",
        "    print(batch.shape)\n",
        "    return batch.transpose(1, 0, 2, 3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WGwXWD27zbnm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_im2col_indices(x_shape, field_height, field_width, padding=1, stride=1):\n",
        "    # First figure out what the size of the output should be\n",
        "    N, C, H, W = x_shape\n",
        "    assert (H + 2 * padding - field_height) % stride == 0\n",
        "    assert (W + 2 * padding - field_height) % stride == 0\n",
        "    out_height = (H + 2 * padding - field_height) / stride + 1\n",
        "    out_width = (W + 2 * padding - field_width) / stride + 1\n",
        "\n",
        "    i0 = np.repeat(np.arange(field_height), field_width)\n",
        "    i0 = np.tile(i0, C)\n",
        "    i1 = stride * np.repeat(np.arange(out_height), out_width)\n",
        "    j0 = np.tile(np.arange(field_width), field_height * C)\n",
        "    j1 = stride * np.tile(np.arange(out_width), out_height)\n",
        "    i = i0.reshape(-1, 1) + i1.reshape(1, -1)\n",
        "    j = j0.reshape(-1, 1) + j1.reshape(1, -1)\n",
        "\n",
        "    k = np.repeat(np.arange(C), field_height * field_width).reshape(-1, 1)\n",
        "\n",
        "    return (k, i, j)\n",
        "\n",
        "\n",
        "def im2col_indices(x, field_height, field_width, padding=1, stride=1):\n",
        "    \"\"\" An implementation of im2col based on some fancy indexing \"\"\"\n",
        "    # Zero-pad the input\n",
        "    p = padding\n",
        "    x_padded = np.pad(x, ((0, 0), (0, 0), (p, p), (p, p)), mode='constant')\n",
        "\n",
        "    k, i, j = get_im2col_indices(x.shape, field_height, field_width, padding,\n",
        "                                 stride)\n",
        "\n",
        "    cols = x_padded[:, k, i, j]\n",
        "    C = x.shape[1]\n",
        "    cols = cols.transpose(1, 2, 0).reshape(field_height * field_width * C, -1)\n",
        "    return cols"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oF1sNZKIzOKz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def conv_forward(X, W, b, stride=1, padding=1):\n",
        "    #X = np.transpose(X, (0, 3, 1, 2))    \n",
        "    #cache = W, b, stride, padding\n",
        "    n_filters, d_filter, h_filter, w_filter = W.shape\n",
        "    n_x, d_x, h_x, w_x = X.shape\n",
        "    assert (h_x - h_filter + 2 * padding) % stride == 0\n",
        "    assert (w_x - w_filter + 2 * padding) % stride == 0\n",
        "    h_out = int((h_x - h_filter + 2 * padding) / stride + 1)\n",
        "    w_out = int((w_x - w_filter + 2 * padding) / stride + 1)\n",
        "    \n",
        "    X_col = im2col_indices(X, h_filter, w_filter, padding=padding, stride=stride)\n",
        "    W_col = W.reshape(n_filters, -1)\n",
        "    \n",
        "    if (bias==True):\n",
        "        out = W_col @ X_col + b\n",
        "\n",
        "    out = out.reshape(n_filters, h_out, w_out, n_x)\n",
        "    out = out.transpose(3, 0, 1, 2)\n",
        "\n",
        "    cache = (X, W, b, stride, padding, X_col)\n",
        "\n",
        "    return out, cache"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q51EEPrwadBT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_layer(h, w, key, channels, scale=1e-2,bias=True):\n",
        "    w_key, b_key = random.split(key)\n",
        "    if (bias==True):\n",
        "        return scale * random.normal(w_key, (h, w)), scale * random.normal(b_key, (1,))\n",
        "    else:\n",
        "        return scale * random.normal(w_key, (h, w)), 0\n",
        "    \n",
        "def create_conv_layer(channels, num_filters, height, width, key, scale=1e-2, bias=True):\n",
        "    filters = []\n",
        "    for n in range(num_filters):\n",
        "        filters.append(create_layer(height, width, key, channels, scale=1e-2,bias=bias))\n",
        "    return np.stack(filters)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jqWQoet3dNp-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Discriminator():\n",
        "    def __init__(self):\n",
        "        self.step_size = 0.0001\n",
        "        self.image_shape = (256,256,3)\n",
        "        self.bias = False\n",
        "        self.params = []\n",
        "        num_layers = 6\n",
        "        num_filters = 64\n",
        "        filter_size = 4\n",
        "        params.append(create_conv_layer(3, \n",
        "                                        num_layers, \n",
        "                                        filter_size, \n",
        "                                        filter_size, \n",
        "                                        random.PRNGKey(0), \n",
        "                                        bias=self.bias))\n",
        "        for l in range(1, num_layers):\n",
        "            params.append(create_conv_layer(64*2**(l-1), \n",
        "                                            64*2**l, \n",
        "                                            filter_size, \n",
        "                                            filter_size, \n",
        "                                            random.PRNGKey(0), \n",
        "                                            bias=self.bias))\n",
        "        params.append(create_conv_layer(64*2**num_filters, \n",
        "                                        1, \n",
        "                                        filter_size, \n",
        "                                        filter_size, \n",
        "                                        random.PRNGKey(0), \n",
        "                                        bias=self.bias))\n",
        "    \n",
        "    def predict():\n",
        "        activations = image\n",
        "        for w, b in params[:-1]:\n",
        "            outputs = conv_forward(activations,w,b,stride=2)\n",
        "            outputs = batch_normalization(outputs)\n",
        "            activations = leaky_relu(outputs)\n",
        "        final_w, final_b = params[-1]\n",
        "        return sigmoid(conv_forward(activations,final_w,final_b,))\n",
        "    \n",
        "    def batched_predict(images):\n",
        "        shape = [None] + list(self.image_shape)\n",
        "        return vmap(self.predict, in_axes=shape)(params, images)\n",
        "    \n",
        "    def loss(params, images, targets):\n",
        "        preds = batched_predict(params, images)\n",
        "        return -np.sum(preds * targets)\n",
        "    \n",
        "    #@jit\n",
        "    def update(params, x, y):\n",
        "        grads = grad(loss)(self.params, x, y)\n",
        "        return [(w - self.step_size * dw, b - self.step_size * db)\n",
        "                for (w, b), (dw, db) in zip(self.params, grads)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rF0d9_S5w6xo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Generator():\n",
        "    def __init__(self):\n",
        "        self.step_size = 0.0001\n",
        "        self.image_shape = (224,224,3)\n",
        "        self.params = []   \n",
        "        \n",
        "    def predict():\n",
        "        pass\n",
        "    \n",
        "    def batched_predict():\n",
        "        pass\n",
        "    \n",
        "    def loss(params, images, targets):\n",
        "        preds = batched_predict(params, images)\n",
        "        return -np.sum(preds * targets)\n",
        "    \n",
        "    #@jit\n",
        "    def update(params, x, y):\n",
        "        grads = grad(loss)(params, x, y)\n",
        "        return [(w - self.step_size * dw, b - self.step_size * db)\n",
        "                for (w, b), (dw, db) in zip(params, grads)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FXAApIpMPgYR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "n_filters = 2\n",
        "field_height=2\n",
        "field_width=2\n",
        "padding=1\n",
        "stride=1\n",
        "N, C, H, W = (3,1,3,3)\n",
        "assert (H + 2 * padding - field_height) % stride == 0\n",
        "assert (W + 2 * padding - field_height) % stride == 0\n",
        "out_height = int((H + 2 * padding - field_height) / stride + 1)\n",
        "out_width = int((W + 2 * padding - field_width) / stride + 1)\n",
        "\n",
        "print(out_height)\n",
        "print(out_width)\n",
        "\n",
        "y = np.repeat(np.ones(n_filters*field_height*field_width), C).reshape((n_filters,field_height,field_width,C)).astype(np.int)\n",
        "y = y.reshape(n_filters, -1)\n",
        "x = np.repeat(np.ones(N*H*W), C).reshape((N,H,W,C)).astype(np.int)\n",
        "x = np.transpose(x, (0, 3, 1, 2))\n",
        "p = padding\n",
        "x_padded = np.pad(x, ((0, 0), (0, 0), (p, p), (p, p)), mode='constant')\n",
        "i0 = np.repeat(np.arange(field_height), field_width)\n",
        "i0 = np.tile(i0, C)\n",
        "i1 = stride * np.repeat(np.arange(out_height), out_width)\n",
        "j0 = np.tile(np.arange(field_width), field_height * C)\n",
        "j1 = stride * np.tile(np.arange(out_width), out_height)\n",
        "i = i0.reshape(-1, 1) + i1.reshape(1, -1)\n",
        "j = j0.reshape(-1, 1) + j1.reshape(1, -1)\n",
        "k = np.repeat(np.arange(C), field_height * field_width).reshape(-1, 1)\n",
        "cols = x_padded[:, k, i, j]\n",
        "C = x.shape[1]\n",
        "cols = cols.transpose(1, 2, 0).reshape(field_height * field_width * C, -1)\n",
        "out = y @ cols\n",
        "\n",
        "print(i0)\n",
        "print(i1)\n",
        "print(j0)\n",
        "print(j1)\n",
        "print(\"-------\")\n",
        "print(k)\n",
        "print(i)\n",
        "print(j)\n",
        "print(C)\n",
        "print(cols)\n",
        "print(cols.shape)\n",
        "print(y.shape)\n",
        "print(out.shape)\n",
        "print(out)\n",
        "print(\"-------\")\n",
        "out = out.reshape(n_filters, out_height, out_width, N)\n",
        "print(out)\n",
        "print(\"-------\")\n",
        "out = out.transpose(3, 0, 1, 2)\n",
        "print(out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HHIg97HTSu9I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x = np.arange(12).reshape((2,2,3))\n",
        "print(x)\n",
        "print(x.shape)\n",
        "y = np.transpose(x, (2, 0, 1))\n",
        "print(y)\n",
        "print(y.shape)\n",
        "\n",
        "x = np.arange(12).reshape((2,2,3))\n",
        "print(x)\n",
        "print(x.shape)\n",
        "y = np.moveaxis(x, 2, 0)\n",
        "print(y)\n",
        "print(y.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yBSE0q5mD2Zm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O7LDBj5oi72i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "i = np.random.randint(5, size=(2, 1, 3, 3))\n",
        "print(i)\n",
        "#print(i.transpose(1, 0, 2, 3))\n",
        "p = batch_normalization(i)\n",
        "print(p.shape)\n",
        "print(p)\n",
        "#print(p.transpose(1, 0, 2, 3))\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}